{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we:\n",
    "\n",
    "1. Load the JSON files from the `json_entity` folder into one place.\n",
    "2. Clean and format in order to upload the dataset to Argilla.\n",
    "3. Update the dataset to Argila for curation.\n",
    "4. Once the dataset has been curated we donwload it and upload it to the HuggingFace Hub."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: remember to install the requirements from the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import argilla as rg\n",
    "import re\n",
    "import spacy  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_4.json',\n",
       " 'output_6.json',\n",
       " 'output_7.json',\n",
       " 'output_8.json',\n",
       " 'output_3.json',\n",
       " 'output_10.json',\n",
       " 'output_9.json',\n",
       " 'output_2.json',\n",
       " 'output_1.json',\n",
       " 'output_11.json',\n",
       " 'output_0.json',\n",
       " 'output_5.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json_folder = \"../../data/NER/json_entity/json_files_alberto/\"\n",
    "json_folder = \"../../data/NER/json_entity/\"\n",
    "json_files = [\n",
    "    file_name for file_name in os.listdir(json_folder) if file_name.endswith(\".json\")\n",
    "]\n",
    "json_files\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the JSON files into a python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for fname in json_files:\n",
    "    with open(json_folder + fname, \"r\") as f:\n",
    "        data.extend(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people',\n",
       " 'none',\n",
       " 'products',\n",
       " 'books',\n",
       " 'animals',\n",
       " 'organizations',\n",
       " 'topic',\n",
       " 'dates',\n",
       " 'places',\n",
       " 'artista',\n",
       " 'None',\n",
       " 'topics',\n",
       " 'objects',\n",
       " 'songs',\n",
       " 'films']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the unique entity classes found in the dataset\n",
    "unique_classes = set()\n",
    "for record in data:\n",
    "    for entity in record[\"entities\"]:\n",
    "        unique_classes.add(entity[\"class\"])\n",
    "unique_classes = list(unique_classes)\n",
    "\n",
    "unique_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sí\n",
      "sí\n",
      "todo\n",
      "magia\n",
      "ya\n",
      "te\n",
      "digo\n",
      "Y así empieza la relación de amistad.\n",
      "Ay\n",
      "todo\n",
      "el\n",
      "rato\n",
      "era\n",
      "un\n",
      "coñazo\n",
      "Pero\n",
      "como\n",
      "si\n",
      "fuera\n",
      "así\n",
      "una\n",
      "cosa\n",
      "de\n",
      "plastilina\n",
      ".\n",
      "¿Por qué han desaparecido las tartas?\n"
     ]
    }
   ],
   "source": [
    "# it is reasonable to discard the entities with class None/none\n",
    "for record in data[:55]:\n",
    "    for entity in record[\"entities\"]:\n",
    "        if entity[\"class\"] == \"None\" or entity[\"class\"] == \"none\":\n",
    "            print(entity[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': 'topics', 'text': 'monstruo'}\n",
      "{'class': 'topics', 'text': 'aniquilador de Almax'}\n",
      "{'class': 'topics', 'text': 'arte'}\n",
      "{'class': 'topics', 'text': 'ocupación alemana en Francia'}\n",
      "{'class': 'topics', 'text': 'guerra'}\n",
      "{'class': 'topic', 'text': 'deseo mimético'}\n",
      "{'class': 'topic', 'text': 'escalada'}\n",
      "{'class': 'topic', 'text': 'deseo compartido'}\n",
      "{'class': 'topic', 'text': 'creencia'}\n",
      "{'class': 'topic', 'text': 'valor del objeto'}\n"
     ]
    }
   ],
   "source": [
    "# also topics and topic seem to be the same category\n",
    "for record in data[:20]:\n",
    "    for entity in record[\"entities\"]:\n",
    "        if entity[\"class\"] == \"topics\":\n",
    "            print(entity)\n",
    "\n",
    "for record in data:\n",
    "    for entity in record[\"entities\"]:\n",
    "        if entity[\"class\"] == \"topic\":\n",
    "            print(entity)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Upload to Argilla server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_position(text, subtext):\n",
    "    \"\"\"\"Finds the start and end position of `subtext` in `text`.\"\"\"\n",
    "    r = re.search(subtext.lower(), text.lower())\n",
    "    if r is not None:\n",
    "        return r.span()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def check_multilabel(label_limits, start, end):\n",
    "    \"\"\"Checks if start and end are inside other label intervals.\"\"\"\n",
    "    for _, start2, end2 in label_limits:\n",
    "        if (start >= start2 and end <= end2) or (start <= start2 and end >= end2):\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_record(record):\n",
    "    \"\"\"Preprocess record in order to be suitable for Argilla.\"\"\"\n",
    "    text = record[\"text\"].strip()\n",
    "    prediction = []\n",
    "    metadata = []\n",
    "\n",
    "    # create list of tuples as (<Entity_name>, <Start_idx>, <End_idx>)\n",
    "    for entity in record[\"entities\"]:\n",
    "        entity_class = entity[\"class\"].upper()\n",
    "        # clean topics class name\n",
    "        if entity_class == \"TOPICS\":\n",
    "            entity_class = \"TOPIC\"\n",
    "        # discard entities with None class\n",
    "        if entity_class == \"NONE\":\n",
    "            continue\n",
    "\n",
    "        entity_text = entity[\"text\"]\n",
    "        span = find_position(text, entity_text)\n",
    "        # if theres no match in the text \"start\" and \"end\" are set to None\n",
    "\n",
    "        if span is not None:\n",
    "            start, end = span\n",
    "            # argilla doesn't support multilabel tokens, so force one label when\n",
    "            # an overlap occurs\n",
    "            if check_multilabel(prediction, start, end) == True:\n",
    "                continue\n",
    "\n",
    "            prediction.append((entity_class, start, end))\n",
    "\n",
    "        metadata.append((entity_class, entity_text))\n",
    "\n",
    "    return text, prediction, metadata\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a list of `TokenClassificationRecord`s for out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "rg_records = []\n",
    "for record in data:\n",
    "    text, prediction, metadata = preprocess_record(record)\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    try:\n",
    "        rg_record = rg.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            prediction=prediction,\n",
    "            prediction_agent=\"ChatGPT\",\n",
    "            metadata={\"ocurrences\": metadata},\n",
    "        )\n",
    "    except:\n",
    "        rg_record = rg.TokenClassificationRecord(\n",
    "            text=text, tokens=tokens, metadata={\"occurences\": metadata}\n",
    "        )\n",
    "    rg_records.append(rg_record)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize connection to the Argilla server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.init(\n",
    "    api_url=\"https://davidfm43-argilla-podcasts-ner.hf.space\", api_key=\"team.apikey\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_dataset_name = \"podcasts-ner-v1\"\n",
    "## This line is commented so you don't upload data to Argilla by accident\n",
    "# rg.log(rg_records, name=rg_dataset_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we curate the dataset from the Argilla GUI client."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Cleaned dataset and upload to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_ds = rg.load(rg_dataset_name, query=\"status:Validated\")\n",
    "hf_ds = hf_ds.to_datasets()\n",
    "hf_ds = hf_ds.select_columns([\"id\", \"text\", \"annotation\"])\n",
    "hf_ds = hf_ds.train_test_split(train_size=0.8, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This line is commented so you don't upload dataset to the HF hub by accident\n",
    "# hf_ds.push_to_hub(\"hackathon-somos-nlp-2023/podcasts-ner-es\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
