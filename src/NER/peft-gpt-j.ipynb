{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-27T05:30:47.303693Z","iopub.status.busy":"2023-03-27T05:30:47.302436Z","iopub.status.idle":"2023-03-27T05:30:53.006261Z","shell.execute_reply":"2023-03-27T05:30:53.005045Z","shell.execute_reply.started":"2023-03-27T05:30:47.303646Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import bitsandbytes as bnb\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n","import transformers\n","from datasets import load_dataset\n","import huggingface_hub\n","import wandb\n","\n","wandb.login()\n","huggingface_hub.login()\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-27T05:30:54.824954Z","iopub.status.busy":"2023-03-27T05:30:54.823509Z","iopub.status.idle":"2023-03-27T05:33:36.774558Z","shell.execute_reply":"2023-03-27T05:33:36.773486Z","shell.execute_reply.started":"2023-03-27T05:30:54.824900Z"},"trusted":true},"outputs":[],"source":["# model_id = \"DavidFM43/bertin-gpt-j-6b-half-sharded\"\n","model_id = \"bertin-project/bertin-gpt-j-6B\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    revision=\"half\",\n","    load_in_8bit=True,\n","    device_map=\"auto\",\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-27T05:34:12.982068Z","iopub.status.busy":"2023-03-27T05:34:12.981594Z","iopub.status.idle":"2023-03-27T05:34:18.440299Z","shell.execute_reply":"2023-03-27T05:34:18.439263Z","shell.execute_reply.started":"2023-03-27T05:34:12.982020Z"},"trusted":true},"outputs":[],"source":["tokenizer.pad_token = tokenizer.eos_token\n","new_tokens = [\"<SP>\", \"<EP>\"]\n","\n","num_added_toks = tokenizer.add_tokens(new_tokens)\n","print(\"We have added\", num_added_toks, \"tokens\")\n","# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n","model.resize_token_embeddings(len(tokenizer))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-27T05:34:18.442789Z","iopub.status.busy":"2023-03-27T05:34:18.442095Z","iopub.status.idle":"2023-03-27T05:34:18.607803Z","shell.execute_reply":"2023-03-27T05:34:18.605794Z","shell.execute_reply.started":"2023-03-27T05:34:18.442747Z"},"trusted":true},"outputs":[],"source":["from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n","\n","\n","model = prepare_model_for_int8_training(model)\n","\n","\n","def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","\n","\n","config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","model = get_peft_model(model, config)\n","print_trainable_parameters(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-27T05:34:25.776190Z","iopub.status.busy":"2023-03-27T05:34:25.775063Z","iopub.status.idle":"2023-03-27T05:34:29.297561Z","shell.execute_reply":"2023-03-27T05:34:29.296592Z","shell.execute_reply.started":"2023-03-27T05:34:25.776120Z"},"trusted":true},"outputs":[],"source":["dataset_id = \"hackathon-somos-nlp-2023/podcasts-ner-es\"\n","dataset = load_dataset(dataset_id, use_auth_token=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def format_ds(example):\n","    text = example[\"text\"]\n","    ents = []\n","    for ent in example[\"annotation\"]:\n","        label = ent[\"label\"]\n","        ent_text = text[ent[\"start\"] : ent[\"end\"]]\n","        ents.append(f\"({label}, {ent_text})\")\n","    entities_text = \", \".join(ents)\n","    example[\"prompt\"] = f\"<SP> text: {text}\\n\\n entities: {entities_text} <EP>\"\n","\n","    return example\n","\n","\n","dataset = dataset.map(format_ds, remove_columns=[\"id\", \"text\", \"annotation\"])\n","dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-27T05:36:33.276097Z","iopub.status.busy":"2023-03-27T05:36:33.275712Z","iopub.status.idle":"2023-03-27T05:44:27.062020Z","shell.execute_reply":"2023-03-27T05:44:27.060287Z","shell.execute_reply.started":"2023-03-27T05:36:33.276060Z"},"trusted":true},"outputs":[],"source":["trainer = transformers.Trainer(\n","    model=model,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    args=transformers.TrainingArguments(\n","        per_device_train_batch_size=4,\n","        gradient_accumulation_steps=8,\n","        warmup_steps=100,\n","        max_steps=1000,\n","        learning_rate=2e-4,\n","        fp16=True,\n","        logging_steps=5,\n","        output_dir=\"outputs\",\n","        report_to=\"wandb\",\n","    ),\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")\n","\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","\n","trainer.train()\n","\n","model.push_to_hub(\n","    \"hackathon-somos-nlp-2023/bertin-gpt-j-6b-ner-es\", use_auth_token=True\n",")\n","tokenizer.push_to_hub(\n","    \"hackathon-somos-nlp-2023/bertin-gpt-j-6b-ner-es\", use_auth_token=True\n",")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
